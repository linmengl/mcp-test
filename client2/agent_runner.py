from typing import List, Dict, Optional

import util
from logger_util import setup_logger
from prompt_manager import PromptManager
from tool_call_manager import ToolCallManager
from util import ollama_chat

logger = setup_logger("AgentRunner")


class AgentRunner:
    def __init__(self, session, behavior_mode="auto"):
        self.session = session
        self.prompt_mgr = PromptManager()
        self.behavior_mode = behavior_mode
        self.tool_parser = ToolCallManager()

    async def run(self, query: str) -> str:
        self.prompt_mgr.reset()
        self.prompt_mgr.add_system(await self._build_system_prompt())
        self.prompt_mgr.add_user(query)

        while True:
            messages = self.prompt_mgr.get_messages()
            logger.debug("ğŸ”„ Sending messages to model.messages: %s", messages)
            llm_response = await ollama_chat(messages)
            llm_content = llm_response["message"]["content"]
            logger.debug("Model response: %s", llm_content)

            # å…è®¸ä¸­é€”åˆ‡æ¢æ¨¡å¼
            if mode := self._check_dynamic_mode_switch(llm_content):
                self.behavior_mode = mode
                logger.info("ğŸ” Behavior mode switched to: %s", mode)
                continue

            if "Final Answer:" in llm_content:
                self.prompt_mgr.add_assistant_message(llm_content)
                return llm_content.split("Final Answer:", 1)[1].strip()

            tool_calls = self.tool_parser.parse_tool_calls(llm_content)
            logger.info("è§£æéœ€è¦è°ƒç”¨çš„å·¥å…·.tool_calls: %s", tool_calls)
            self.prompt_mgr.add_assistant_message(llm_content)

            if not tool_calls:
                logger.info("âš ï¸ æ— æ³•è¯†åˆ«å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ¨¡å‹å›å¤")
                return llm_content  # fallback to LLM response

            # tool_outputs = []
            for call in tool_calls:
                try:
                    result = await self.session.call_tool(call.get("tool"), call.get("arguments"))
                    tool_output = parse_tool_result(result)
                    logger.info("å·¥å…·è¿”å›ç»“æœ.å·¥å…·å: %s,result: %s", call.get("tool"), tool_output)
                    self.prompt_mgr.add_tool_result(tool_output, call.get("tool"))
                    # tool_outputs.append({"role": call.get("tool"), "result": tool_output, "tool_call_id": str(hash(call.get("tool")))})
                except Exception as e:
                    error = {"error": str(e)}
                    self.prompt_mgr.add_tool_result(error, call.get("tool"))
                    logger.info("å·¥å…·è°ƒç”¨å¼‚å¸¸.å·¥å…·å: %s,result: %s", call.get("tool"), error)
                    # tool_outputs.append({"tool": call["tool"], "result": error})

            # logger.info("ğŸ”§ å·¥å…·è°ƒç”¨ç»“æœ", tool_outputs)

    async def _get_tools(self) -> List[Dict]:
        tool_resp = await self.session.list_tools()
        return [{
            "name": t.name,
            "description": t.description,
            "input_schema": t.inputSchema
        } for t in tool_resp.tools]

    def _build_system_prompt(self, tools_prompt: str) -> str:
        base = (
            f"ä½ æ˜¯ä¸€ä¸ªèªæ˜ã€å¯é ã€éµå¾ªè§„åˆ™çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥è®¿é—®å¦‚ä¸‹å·¥å…·ï¼š\n"
            f"{tools_prompt}\n\n---\n"
            "ğŸ“Œ ä½¿ç”¨è§„åˆ™ï¼š\n"

            "âœ… 1. å¦‚æœä½ å¯ä»¥ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼ˆä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼‰ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼å›å¤ï¼š\n"
            "Final Answer: [ä½ çš„æœ€ç»ˆå›ç­”]\n"
            "ç¤ºä¾‹ï¼š\n"
            "Final Answer: ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸© 25Â°Cã€‚\n"
            "ğŸ”§ 2. å¦‚æœä½ éœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›å¤ï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š\n"

            "```json\n"
            "{{\n"
            "  \"tool\": \"å·¥å…·å\",\n"
            "  \"arguments\": {{\n"
            "    \"å‚æ•°1\": \"å€¼\",\n"
            "    \"å‚æ•°2\": \"å€¼\"\n"
            "  }}\n"
            "}}\n```\n"
        )

        if self.behavior_mode == "tool-first":
            base += "\nğŸ’¡ å½“å‰ä¸º Tool-First æ¨¡å¼ï¼šè¯·ä¼˜å…ˆåˆ¤æ–­æ˜¯å¦å¯è°ƒç”¨å·¥å…·è§£å†³é—®é¢˜ï¼Œå†è€ƒè™‘ç›´æ¥å›ç­”ã€‚"
        elif self.behavior_mode == "llm-first":
            base += "\nğŸ§  å½“å‰ä¸º LLM-First æ¨¡å¼ï¼šè¯·å°½é‡ç›´æ¥å›ç­”é—®é¢˜ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨å·¥å…·ã€‚"
        else:
            base += "\nâš™ï¸ å½“å‰ä¸º Auto æ¨¡å¼ï¼šä½ å¯ä»¥è‡ªè¡Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨å·¥å…·ã€‚"

        return base

    def _check_dynamic_mode_switch(self, response: str) -> Optional[str]:
        lowered = response.lower()
        if "mode:tool-first" in lowered:
            return "tool-first"
        elif "mode:llm-first" in lowered:
            return "llm-first"
        elif "mode:auto" in lowered:
            return "auto"
        return None

    async def _build_system_prompt(self) -> str:
        tool_response = await self.session.list_tools()
        tools = [{
            "name": t.name,
            "description": t.description,
            "input_schema": t.inputSchema
        } for t in tool_response.tools]
        return util.build_system_prompt(tools)


def parse_tool_result(result) -> str:
    """
    æå– MCP å·¥å…·è¿”å›ç»“æœä¸­çš„æ–‡æœ¬å†…å®¹ã€‚

    Args:
        result: å·¥å…·è°ƒç”¨è¿”å›ç»“æœï¼ˆä¸€èˆ¬å«æœ‰ .content, .isError ç­‰å±æ€§ï¼‰

    Returns:
        æ ¼å¼åŒ–åçš„çº¯æ–‡æœ¬å­—ç¬¦ä¸²ã€‚å¦‚æœå‡ºé”™ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ã€‚
    """
    if getattr(result, "isError", False):
        return "[âŒ å·¥å…·è°ƒç”¨å¤±è´¥]"

    content_blocks = getattr(result, "content", [])
    if not content_blocks:
        return "[âš ï¸ å·¥å…·æœªè¿”å›ä»»ä½•å†…å®¹]"

    texts = []
    for block in content_blocks:
        text = getattr(block, "text", None)
        if text:
            texts.append(text.strip())

    return "\n\n".join(texts) if texts else "[âš ï¸ å·¥å…·è¿”å›å†…å®¹æ— æ³•è§£æ]"
